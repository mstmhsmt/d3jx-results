// generated by Diff/AST Java Unparser
package org.deeplearning4j.nn.multilayer;
import com.sun.org.apache.xpath.internal.operations.*;
import org.apache.commons.lang3.ArrayUtils;
import org.deeplearning4j.berkeley.Pair;
import org.deeplearning4j.eval.Evaluation;
import org.deeplearning4j.nn.api.*;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.gradient.DefaultGradient;
import org.deeplearning4j.nn.gradient.Gradient;
import org.deeplearning4j.nn.layers.OutputLayer;
import org.deeplearning4j.nn.layers.factory.LayerFactories;
import org.deeplearning4j.nn.params.DefaultParamInitializer;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.api.ConvexOptimizer;
import org.deeplearning4j.optimize.api.IterationListener;
import org.deeplearning4j.util.MultiLayerUtil;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.NDArrayIndex;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.nd4j.linalg.ops.transforms.Transforms;
import org.nd4j.linalg.util.FeatureUtil;
import org.nd4j.linalg.util.LinAlgExceptions;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import java.io.Serializable;
import java.lang.String;
import java.lang.reflect.Constructor;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
public class MultiLayerNetwork implements Serializable, Classifier {
  private static final Logger log =
      LoggerFactory.getLogger(MultiLayerNetwork.class);
  private static final long serialVersionUID = -5029161847383716484L;
  protected Layer[] layers;
  protected INDArray input, labels;
  protected boolean initCalled = false;
  private List<IterationListener> listeners = new ArrayList<>();
  protected NeuralNetConfiguration defaultConfiguration;
  protected MultiLayerConfiguration layerWiseConfigurations;
  protected Map<Integer, Object> multiGradientAndScore = new HashMap<>();
  protected Gradient gradient;
  protected double score;
  protected INDArray mask;
  public MultiLayerNetwork(MultiLayerConfiguration conf) {
    this.layerWiseConfigurations = conf;
    this.defaultConfiguration = conf.getConf(0);
  }
  public MultiLayerNetwork(String conf, INDArray params) {
    this(MultiLayerConfiguration.fromJson(conf));
    init();
    setParameters(params);
  }
  public MultiLayerNetwork(MultiLayerConfiguration conf, INDArray params) {
    this(conf);
    init();
    setParameters(params);
  }
  protected void intializeConfigurations() {
    if (layerWiseConfigurations == null)
      layerWiseConfigurations = new MultiLayerConfiguration.Builder().build();
    if (layers == null)
      layers = new Layer[getnLayers()];
    if (defaultConfiguration == null)
      defaultConfiguration = new NeuralNetConfiguration.Builder().build();
    if (layerWiseConfigurations == null ||
        layerWiseConfigurations.getConfs().isEmpty())
      for (int i = 0;
           i < layerWiseConfigurations.getHiddenLayerSizes().length + 1; i++) {
        layerWiseConfigurations.getConfs().add(defaultConfiguration.clone());
      }
  }
  public void pretrain(DataSetIterator iter) {
    if (!layerWiseConfigurations.isPretrain())
      return;
    INDArray layerInput;
    for (int i = 0; i < getnLayers(); i++) {
      if (i == 0) {
        while (iter.hasNext()) {
          DataSet next = iter.next();
          this.input = next.getFeatureMatrix();
          if (this.getInput() == null || this.getLayers() == null) {
            setInput(input);
            initializeLayers(input);
          } else
            setInput(input);
          getLayers()[i].fit(this.input);
          log.info("Training on layer " + (i + 1) + " with " + input.slices() +
                   " examples");
        }
        iter.reset();
      } else {
        while (iter.hasNext()) {
          DataSet next = iter.next();
          layerInput = next.getFeatureMatrix();
          for (int j = 1; j <= i; j++)
            layerInput = activationFromPrevLayer(j - 1, layerInput, true);
          log.info("Training on layer " + (i + 1) + " with " +
                   layerInput.slices() + " examples");
          getLayers()[i].fit(layerInput);
        }
        iter.reset();
      }
    }
  }
  public void pretrain(INDArray input) {
    if (!layerWiseConfigurations.isPretrain())
      return;
    INDArray layerInput = null;
    for (int i = 0; i < getnLayers() - 1; i++) {
      if (i == 0)
        layerInput = input;
      else
        layerInput = activationFromPrevLayer(i - 1, layerInput, true);
      log.info("Training on layer " + (i + 1) + " with " + layerInput.slices() +
               " examples");
      getLayers()[i].fit(layerInput);
    }
  }
  @Override
  public int batchSize() {
    return input.slices();
  }
  @Override
  public NeuralNetConfiguration conf() {
    throw new UnsupportedOperationException();
  }
  @Override
  public void setConf(NeuralNetConfiguration conf) {
    throw new UnsupportedOperationException();
  }
  @Override
  public INDArray input() {
    return input;
  }
  @Override
  public void validateInput() {}
  @Override
  public ConvexOptimizer getOptimizer() {
    throw new UnsupportedOperationException();
  }
  @Override
  public INDArray getParam(String param) {
    throw new UnsupportedOperationException();
  }
  @Override
  public void initParams() {
    throw new UnsupportedOperationException();
  }
  @Override
  public Map<String, INDArray> paramTable() {
    throw new UnsupportedOperationException();
  }
  @Override
  public void setParamTable(Map<String, INDArray> paramTable) {
    throw new UnsupportedOperationException();
  }
  @Override
  public void setParam(String key, INDArray val) {
    throw new UnsupportedOperationException();
  }
  @Override
  public INDArray transform(INDArray data) {
    return output(data);
  }
  public MultiLayerConfiguration getLayerWiseConfigurations() {
    return layerWiseConfigurations;
  }
  public void
  setLayerWiseConfigurations(MultiLayerConfiguration layerWiseConfigurations) {
    this.layerWiseConfigurations = layerWiseConfigurations;
  }
  public void initializeLayers(INDArray input) {
    if (input == null)
      throw new IllegalArgumentException(
          "Unable to initialize neuralNets with empty input");
    int[] hiddenLayerSizes = getLayerWiseConfigurations().getHiddenLayerSizes();
    if (input.shape().length == 2)
      for (int i = 0; i < hiddenLayerSizes.length; i++)
        if (hiddenLayerSizes[i] < 1)
          throw new IllegalArgumentException(
              "All hidden layer sizes must be >= 1");
    this.input = input;
    if (!initCalled)
      init();
  }
  public void init() {
    if (layerWiseConfigurations == null || layers == null)
      intializeConfigurations();
    if (initCalled)
      return;
    INDArray layerInput = input();
    int inputSize;
    if (getnLayers() < 1)
      throw new IllegalStateException(
          "Unable to createComplex network neuralNets; number specified is " +
          "less than 1");
    int[] hiddenLayerSizes = layerWiseConfigurations.getHiddenLayerSizes();
    int numHiddenLayersSizesUsed = 0;
    if (this.layers == null || this.layers[0] == null) {
      if (this.layers == null)
        this.layers = new Layer[getnLayers()];
      for (int i = 0; i < getnLayers(); i++) {
        NeuralNetConfiguration conf = layerWiseConfigurations.getConf(i);
        Layer.Type type = LayerFactories.typeForFactory(conf);
        if (i == 0) {
          inputSize = conf.getNIn();
          if (input == null) {
            input = Nd4j.ones(inputSize);
            layerInput = input;
          }
          conf.setNIn(inputSize);
          if (type == Layer.Type.FEED_FORWARD || type == Layer.Type.RECURRENT) {
            conf.setNOut(hiddenLayerSizes[numHiddenLayersSizesUsed]);
          }
        } else if (i < getLayers().length) {
          if (input != null)
            layerInput = activationFromPrevLayer(i - 1, layerInput, true);
          if (type == Layer.Type.FEED_FORWARD || type == Layer.Type.RECURRENT) {
            if (i != (layers.length - 1)) {
              numHiddenLayersSizesUsed++;
              conf.setNIn(layerInput.size(1));
              conf.setNOut(hiddenLayerSizes[numHiddenLayersSizesUsed]);
            } else {
              conf.setNIn(hiddenLayerSizes[numHiddenLayersSizesUsed]);
            }
          }
        }
        layers[i] = LayerFactories.getFactory(conf).create(conf, listeners, i);
      }
      initCalled = true;
      initMask();
    }
  }
  public INDArray activate() {
    return getLayers()[getLayers().length - 1].activate();
  }
  public INDArray activate(int layer) { return getLayers()[layer].activate(); }
  public INDArray activate(int layer, INDArray input) {
    return getLayers()[layer].activate(input);
  }
  public void initialize(DataSet data) {
    setInput(data.getFeatureMatrix());
    feedForward(getInput());
    this.labels = data.getLabels();
    if (getOutputLayer() instanceof OutputLayer) {
      OutputLayer o = (OutputLayer)getOutputLayer();
      o.setLabels(labels);
    }
  }
  public INDArray zFromPrevLayer(int curr, INDArray input, boolean training) {
    if (getLayerWiseConfigurations().getInputPreProcess(curr) != null)
      input = getLayerWiseConfigurations().getInputPreProcess(curr).preProcess(
          input);
    INDArray ret = layers[curr].preOutput(input, training);
    if (getLayerWiseConfigurations().getProcessors() != null &&
        getLayerWiseConfigurations().getPreProcessor(curr) != null) {
      ret = getLayerWiseConfigurations().getPreProcessor(curr).preProcess(ret);
      return ret;
    }
    return ret;
  }
  public INDArray activationFromPrevLayer(int curr, INDArray input,
                                          boolean training) {
    if (getLayerWiseConfigurations().getInputPreProcess(curr) != null)
      input = getLayerWiseConfigurations().getInputPreProcess(curr).preProcess(
          input);
    INDArray ret = layers[curr].activate(input, training);
    if (getLayerWiseConfigurations().getProcessors() != null &&
        getLayerWiseConfigurations().getPreProcessor(curr) != null) {
      ret = getLayerWiseConfigurations().getPreProcessor(curr).preProcess(ret);
      return ret;
    }
    return ret;
  }
  public List<INDArray> computeZ(boolean training) {
    INDArray currInput = this.input;
    List<INDArray> activations = new ArrayList<>();
    activations.add(currInput);
    for (int i = 0; i < layers.length; i++) {
      currInput = zFromPrevLayer(i, currInput, training);
      activations.add(currInput);
    }
    return activations;
  }
  public List<INDArray> computeZ(INDArray input, boolean training) {
    if (input == null)
      throw new IllegalStateException(
          "Unable to perform feed forward; no input found");
    else if (this.getLayerWiseConfigurations().getInputPreProcess(0) != null)
      this.input =
          getLayerWiseConfigurations().getInputPreProcess(0).preProcess(input);
    else
      this.input = input;
    return computeZ(training);
  }
  public List<INDArray> feedForward(INDArray input, boolean test) {
    this.input = input;
    return feedForward(test);
  }
  public List<INDArray> feedForward(boolean test) {
    INDArray currInput = this.input;
    List<INDArray> activations = new ArrayList<>();
    activations.add(currInput);
    for (int i = 0; i < layers.length; i++) {
      currInput = activationFromPrevLayer(i, currInput, test);
      activations.add(currInput);
    }
    return activations;
  }
  public List<INDArray> feedForward() { return feedForward(false); }
  public Pair<List<INDArray>, List<INDArray>>
  feedForwardActivationsAndDerivatives(boolean training) {
    INDArray currInput = this.input;
    List<INDArray> activations = new ArrayList<>();
    List<INDArray> derivatives = new ArrayList<>();
    activations.add(currInput);
    for (int i = 0; i < layers.length; i++) {
      currInput = zFromPrevLayer(i, currInput, training);
      if (layers[i].conf().getActivationFunction().equals("softmax"))
        activations.add(Nd4j.getExecutioner().execAndReturn(
            Nd4j.getOpFactory().createTransform("softmax", currInput.dup()),
            1));
      else
        activations.add(Nd4j.getExecutioner().execAndReturn(
            Nd4j.getOpFactory().createTransform(
                layerWiseConfigurations.getConf(i).getActivationFunction(),
                currInput)));
    }
    currInput = this.input;
    for (int i = 0; i < layers.length; i++) {
      currInput = zFromPrevLayer(i, currInput, training);
      INDArray dup = currInput.dup();
      if (layers[i].conf().getActivationFunction().equals("softmax"))
        derivatives.add(Nd4j.getExecutioner().execAndReturn(
            Nd4j.getOpFactory()
                .createTransform(
                    layerWiseConfigurations.getConf(i).getActivationFunction(),
                    dup)
                .derivative(),
            1));
      else
        derivatives.add(Nd4j.getExecutioner().execAndReturn(
            Nd4j.getOpFactory()
                .createTransform(
                    layerWiseConfigurations.getConf(i).getActivationFunction(),
                    dup)
                .derivative()));
    }
    derivatives.add(derivatives.get(layers.length - 1));
    return new Pair<>(activations, derivatives);
  }
  public List<INDArray> feedForward(INDArray input) {
    if (input == null)
      throw new IllegalStateException(
          "Unable to perform feed forward; no input found");
    else if (this.getLayerWiseConfigurations().getInputPreProcess(0) != null)
      this.input =
          getLayerWiseConfigurations().getInputPreProcess(0).preProcess(input);
    else
      this.input = input;
    return feedForward();
  }
  @Override
  public Gradient gradient() {
    Gradient ret = new DefaultGradient();
    for (int i = 0; i < layers.length; i++) {
      ret.gradientForVariable().put(String.valueOf(i),
                                    layers[i].gradient().gradient());
    }
    return ret;
  }
  @Override
  public Pair<Gradient, Double> gradientAndScore() {
    return new Pair<>(gradient(), getOutputLayer().score());
  }
  @Deprecated
  protected void applyDropConnectIfNecessary(INDArray input) {
    if (layerWiseConfigurations.isUseDropConnect()) {
      INDArray mean = Nd4j.valueArrayOf(input.slices(), input.columns(), 0.5);
      INDArray mask =
          Nd4j.getDistributions().createBinomial(1, mean).sample(mean.shape());
      input.muli(mask);
      if (defaultConfiguration.getL2() > 0)
        input.muli(defaultConfiguration.getL2());
    }
  }
  protected List<INDArray> computeDeltasR(INDArray v) {
    List<INDArray> deltaRet = new ArrayList<>();
    INDArray[] deltas = new INDArray[getnLayers() + 1];
    List<INDArray> activations = feedForward();
    List<INDArray> rActivations = feedForwardR(activations, v);
    List<INDArray> weights = new ArrayList<>();
    List<INDArray> biases = new ArrayList<>();
    List<String> activationFunctions = new ArrayList<>();
    for (int j = 0; j < getLayers().length; j++) {
      weights.add(getLayers()[j].getParam(DefaultParamInitializer.WEIGHT_KEY));
      biases.add(getLayers()[j].getParam(DefaultParamInitializer.BIAS_KEY));
      activationFunctions.add(getLayers()[j].conf().getActivationFunction());
    }
    INDArray rix =
        rActivations.get(rActivations.size() - 1).divi((double)input.slices());
    LinAlgExceptions.assertValidNum(rix);
    for (int i = getnLayers() - 1; i >= 0; i--) {
      deltas[i] = activations.get(i).transpose().mmul(rix);
      if (i > 0)
        rix = rix.mmul(weights.get(i).addRowVector(biases.get(i)).transpose())
                  .muli(Nd4j.getExecutioner().execAndReturn(
                      Nd4j.getOpFactory()
                          .createTransform(activationFunctions.get(i - 1),
                                           activations.get(i))
                          .derivative()));
    }
    for (int i = 0; i < deltas.length - 1; i++) {
      if (defaultConfiguration.isConstrainGradientToUnitNorm()) {
        double sum = deltas[i].sum(Integer.MAX_VALUE).getDouble(0);
        if (sum > 0)
          deltaRet.add(deltas[i].div(deltas[i].norm2(Integer.MAX_VALUE)));
        else
          deltaRet.add(deltas[i]);
      } else
        deltaRet.add(deltas[i]);
      LinAlgExceptions.assertValidNum(deltaRet.get(i));
    }
    return deltaRet;
  }
  public void dampingUpdate(double rho, double boost, double decrease) {
    if (rho < 0.25 || Double.isNaN(rho))
      layerWiseConfigurations.setDampingFactor(
          getLayerWiseConfigurations().getDampingFactor() * boost);
    else if (rho > 0.75)
      layerWiseConfigurations.setDampingFactor(
          getLayerWiseConfigurations().getDampingFactor() * decrease);
  }
  public double reductionRatio(INDArray p, double currScore, double score,
                               INDArray gradient) {
    double currentDamp = layerWiseConfigurations.getDampingFactor();
    layerWiseConfigurations.setDampingFactor(0);
    INDArray denom = getBackPropRGradient(p);
    denom.muli(0.5).muli(p.mul(denom)).sum(0);
    denom.subi(gradient.mul(p).sum(0));
    double rho = (currScore - score) / (double)denom.getScalar(0).element();
    layerWiseConfigurations.setDampingFactor(currentDamp);
    if (score - currScore > 0)
      return Float.NEGATIVE_INFINITY;
    return rho;
  }
  protected List<Pair<INDArray, INDArray>> computeDeltas2() {
    List<Pair<INDArray, INDArray>> deltaRet = new ArrayList<>();
    List<INDArray> activations = feedForward();
    INDArray[] deltas = new INDArray[activations.size() - 1];
    INDArray[] preCons = new INDArray[activations.size() - 1];
    INDArray ix = activations.get(activations.size() - 1)
                      .sub(labels)
                      .div(labels.slices());
    List<INDArray> weights = new ArrayList<>();
    List<INDArray> biases = new ArrayList<>();
    List<String> activationFunctions = new ArrayList<>();
    for (int j = 0; j < getLayers().length; j++) {
      weights.add(getLayers()[j].getParam(DefaultParamInitializer.WEIGHT_KEY));
      biases.add(getLayers()[j].getParam(DefaultParamInitializer.BIAS_KEY));
      activationFunctions.add(getLayers()[j].conf().getActivationFunction());
    }
    for (int i = weights.size() - 1; i >= 0; i--) {
      deltas[i] = activations.get(i).transpose().mmul(ix);
      preCons[i] = Transforms.pow(activations.get(i).transpose(), 2)
                       .mmul(Transforms.pow(ix, 2))
                       .muli(labels.slices());
      if (i > 0) {
        ix = ix.mmul(weights.get(i).transpose())
                 .muli(Nd4j.getExecutioner().execAndReturn(
                     Nd4j.getOpFactory()
                         .createTransform(activationFunctions.get(i - 1),
                                          activations.get(i))
                         .derivative()));
      }
    }
    for (int i = 0; i < deltas.length; i++) {
      if (defaultConfiguration.isConstrainGradientToUnitNorm())
        deltaRet.add(new Pair<>(
            deltas[i].divi(deltas[i].norm2(Integer.MAX_VALUE)), preCons[i]));
      else
        deltaRet.add(new Pair<>(deltas[i], preCons[i]));
    }
    return deltaRet;
  }
  public INDArray getBackPropRGradient(INDArray v) {
    return pack(backPropGradientR(v));
  }
  public Pair<INDArray, INDArray> getBackPropGradient2() {
    List<Pair<Pair<INDArray, INDArray>, Pair<INDArray, INDArray>>> deltas =
        backPropGradient2();
    List<Pair<INDArray, INDArray>> deltaNormal = new ArrayList<>();
    List<Pair<INDArray, INDArray>> deltasPreCon = new ArrayList<>();
    for (int i = 0; i < deltas.size(); i++) {
      deltaNormal.add(deltas.get(i).getFirst());
      deltasPreCon.add(deltas.get(i).getSecond());
    }
    return new Pair<>(pack(deltaNormal), pack(deltasPreCon));
  }
  @Override
  public MultiLayerNetwork clone() {
    MultiLayerNetwork ret;
    try {
      Constructor<MultiLayerNetwork> constructor =
          (Constructor<MultiLayerNetwork>)getClass().getDeclaredConstructor(
              MultiLayerConfiguration.class);
      ret = constructor.newInstance(getLayerWiseConfigurations());
      ret.update(this);
    } catch (Exception e) {
      throw new IllegalStateException("Unable to cloe network");
    }
    return ret;
  }
  @Override
  public INDArray params() {
    List<INDArray> params = new ArrayList<>();
    for (int i = 0; i < getnLayers(); i++)
      params.add(layers[i].params());
    return Nd4j.toFlattened(params);
  }
  @Override
  public void setParams(INDArray params) {
    setParameters(params);
  }
  @Override
  public int numParams() {
    int length = 0;
    for (int i = 0; i < layers.length; i++)
      length += layers[i].numParams();
    return length;
  }
  public INDArray pack() { return params(); }
  public INDArray pack(List<Pair<INDArray, INDArray>> layers) {
    List<INDArray> list = new ArrayList<>();
    for (Pair<INDArray, INDArray> layer : layers) {
      list.add(layer.getFirst());
      list.add(layer.getSecond());
    }
    return Nd4j.toFlattened(list);
  }
  @Override
  public double score(org.nd4j.linalg.dataset.api.DataSet data) {
    return score(data.getFeatureMatrix(), data.getLabels());
  }
  public List<Pair<INDArray, INDArray>> unPack(INDArray param) {
    if (param.slices() != 1)
      param = param.reshape(1, param.length());
    List<Pair<INDArray, INDArray>> ret = new ArrayList<>();
    int curr = 0;
    for (int i = 0; i < layers.length; i++) {
      int layerLength =
          layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).length() +
          layers[i].getParam(DefaultParamInitializer.BIAS_KEY).length();
      INDArray subMatrix =
          param.get(NDArrayIndex.interval(curr, curr + layerLength));
      INDArray weightPortion = subMatrix.get(NDArrayIndex.interval(
          0, layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).length()));
      int beginHBias =
          layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).length();
      int endHbias = subMatrix.length();
      INDArray hBiasPortion =
          subMatrix.get(NDArrayIndex.interval(beginHBias, endHbias));
      int layerLengthSum = weightPortion.length() + hBiasPortion.length();
      if (layerLengthSum != layerLength) {
        if (hBiasPortion.length() !=
            layers[i].getParam(DefaultParamInitializer.BIAS_KEY).length())
          throw new IllegalStateException("Hidden bias on layer " + i +
                                          " was off");
        if (weightPortion.length() !=
            layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).length())
          throw new IllegalStateException("Weight portion on layer " + i +
                                          " was off");
      }
      ret.add(new Pair<>(
          weightPortion.reshape(
              layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).slices(),
              layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).columns()),
          hBiasPortion.reshape(
              layers[i].getParam(DefaultParamInitializer.BIAS_KEY).slices(),
              layers[i].getParam(DefaultParamInitializer.BIAS_KEY).columns())));
      curr += layerLength;
    }
    return ret;
  }
  protected List<Pair<Pair<INDArray, INDArray>, Pair<INDArray, INDArray>>>
  backPropGradient2() {
    List<Pair<INDArray, INDArray>> deltas = computeDeltas2();
    List<Pair<Pair<INDArray, INDArray>, Pair<INDArray, INDArray>>> list =
        new ArrayList<>();
    List<Pair<INDArray, INDArray>> grad = new ArrayList<>();
    List<Pair<INDArray, INDArray>> preCon = new ArrayList<>();
    for (int l = 0; l < deltas.size(); l++) {
      INDArray gradientChange = deltas.get(l).getFirst();
      INDArray preConGradientChange = deltas.get(l).getSecond();
      if (l < layers.length &&
          gradientChange.length() !=
              layers[l].getParam(DefaultParamInitializer.WEIGHT_KEY).length())
        throw new IllegalStateException(
            "Gradient change not equal to weight change");
      INDArray deltaColumnSums = deltas.get(l).getFirst().mean(0);
      INDArray preConColumnSums = deltas.get(l).getSecond().mean(0);
      grad.add(new Pair<>(gradientChange, deltaColumnSums));
      preCon.add(new Pair<>(preConGradientChange, preConColumnSums));
      if (l < layers.length &&
          deltaColumnSums.length() !=
              layers[l].getParam(DefaultParamInitializer.BIAS_KEY).length())
        throw new IllegalStateException(
            "Bias change not equal to weight change");
      else if (l == getLayers().length &&
               deltaColumnSums.length() !=
                   getOutputLayer()
                       .getParam(DefaultParamInitializer.BIAS_KEY)
                       .length())
        throw new IllegalStateException(
            "Bias change not equal to weight change");
    }
    INDArray g = pack(grad);
    INDArray con = pack(preCon);
    INDArray theta = params();
    if (mask == null)
      initMask();
    g.addi(theta.mul(defaultConfiguration.getL2()).muli(mask));
    INDArray conAdd = Transforms.pow(
        mask.mul(defaultConfiguration.getL2())
            .add(Nd4j.valueArrayOf(g.slices(), g.columns(),
                                   layerWiseConfigurations.getDampingFactor())),
        3.0 / 4.0);
    con.addi(conAdd);
    List<Pair<INDArray, INDArray>> gUnpacked = unPack(g);
    List<Pair<INDArray, INDArray>> conUnpacked = unPack(con);
    for (int i = 0; i < gUnpacked.size(); i++)
      list.add(new Pair<>(gUnpacked.get(i), conUnpacked.get(i)));
    return list;
  }
  @Override
  public void fit(DataSetIterator iter) {
    if (layerWiseConfigurations.isPretrain()) {
      pretrain(iter);
      iter.reset();
      finetune(iter);
    }
    if (layerWiseConfigurations.isBackward()) {
      iter.reset();
      while (iter.hasNext()) {
        DataSet next = iter.next();
        setInput(next.getFeatureMatrix());
        setLabels(next.getLabels());
        doBackWard();
      }
    }
  }
  protected void doBackWard() {
    setInput(input);
    if (!(getOutputLayer() instanceof OutputLayer)) {
      log.warn("Warning: final layer isn't output layer. You cannot use " +
               "backprop without an output layer.");
      return;
    }
    List<INDArray> activations = feedForward();
    OutputLayer outputLayer = (OutputLayer)getOutputLayer();
    if (labels == null)
      throw new IllegalStateException("No labels found");
    if (outputLayer.conf().getWeightInit() == WeightInit.ZERO) {
      throw new IllegalStateException(
          "Output layer weights cannot be initialized to zero when using " +
          "backprop.");
    }
    outputLayer.setLabels(labels);
    Gradient nextGradients;
    outputLayer.computeGradientAndScore();
    int numLayers = getnLayers();
    Pair<Gradient, INDArray> pair = outputLayer.backwardGradient(null, null);
    multiGradientAndScore.put(numLayers, outputLayer.gradientAndScore());
    for (int j = numLayers - 2; j >= 0; j--) {
      Layer currLayer = getLayers()[j];
      pair = currLayer.backwardGradient(pair.getFirst(), pair.getSecond());
      multiGradientAndScore.put(j, currLayer.gradientAndScore());
    }
  }
  public List<IterationListener> getListeners() { return listeners; }
  public void setListeners(List<IterationListener> listeners) {
    this.listeners = listeners;
    if (layers == null) {
      init();
    }
    for (Layer layer : layers) {
      layer.setListeners(listeners);
    }
  }
  public void finetune(DataSetIterator iter) {
    log.info("Finetune phase ");
    if (layerWiseConfigurations.isBackward()) {
      log.info("Will use backpropagation for finetune");
      return;
    }
    iter.reset();
    while (iter.hasNext()) {
      DataSet data = iter.next();
      if (data.getFeatureMatrix() == null || data.getLabels() == null)
        break;
      setInput(data.getFeatureMatrix());
      setLabels(data.getLabels());
      if (getOutputLayer().conf().getOptimizationAlgo() !=
          OptimizationAlgorithm.HESSIAN_FREE) {
        feedForward();
        if (getOutputLayer() instanceof OutputLayer) {
          OutputLayer o = (OutputLayer)getOutputLayer();
          o.setListeners(getListeners());
          o.fit(o.input(), getLabels());
        }
      } else {
        throw new UnsupportedOperationException();
      }
    }
  }
  public void finetune(INDArray labels) {
    if (labels != null)
      this.labels = labels;
    if (!(getOutputLayer() instanceof OutputLayer)) {
      log.warn("Output layer not instance of output layer returning.");
      return;
    }
    if (layerWiseConfigurations.isBackward()) {
      log.info("Will use backpropagation for finetune");
      return;
    }
    log.info("Finetune phase");
    OutputLayer o = (OutputLayer)getOutputLayer();
    if (getOutputLayer().conf().getOptimizationAlgo() !=
        OptimizationAlgorithm.HESSIAN_FREE) {
      List<INDArray> activations = feedForward();
      o.setListeners(getListeners());
      o.fit(activations.get(activations.size() - 2), labels);
    } else {
      throw new UnsupportedOperationException();
    }
  }
  @Override
  public int[] predict(INDArray d) {
    INDArray output = output(d);
    int[] ret = new int[d.slices()];
    if (d.isRowVector())
      ret[0] = Nd4j.getBlasWrapper().iamax(output);
    else {
      for (int i = 0; i < ret.length; i++)
        ret[i] = Nd4j.getBlasWrapper().iamax(output.getRow(i));
    }
    return ret;
  }
  @Override
  public INDArray labelProbabilities(INDArray examples) {
    List<INDArray> feed = feedForward(examples);
    OutputLayer o = (OutputLayer)getOutputLayer();
    return o.labelProbabilities(feed.get(feed.size() - 1));
  }
  @Override
  public void fit(INDArray data, INDArray labels) {
    setInput(data.dup());
    if (layerWiseConfigurations.isPretrain()) {
      pretrain(getInput());
      finetune(labels);
    }
    if (layerWiseConfigurations.isBackward())
      setLabels(labels);
    doBackWard();
  }
  @Override
  public void fit(INDArray data) {
    pretrain(data);
  }
  @Override
  public void iterate(INDArray input) {
    pretrain(input);
  }
  @Override
  public void fit(org.nd4j.linalg.dataset.api.DataSet data) {
    fit(data.getFeatureMatrix(), data.getLabels());
  }
  @Override
  public void fit(INDArray examples, int[] labels) {
    fit(examples,
        FeatureUtil.toOutcomeMatrix(labels, getOutputLayer().conf().getNOut()));
  }
  public INDArray output(INDArray x, boolean test) {
    List<INDArray> activations = feedForward(x, test);
    return activations.get(activations.size() - 1);
  }
  public INDArray output(INDArray x) { return output(x, false); }
  public INDArray reconstruct(INDArray x, int layerNum) {
    List<INDArray> forward = feedForward(x);
    return forward.get(layerNum - 1);
  }
  public void printConfiguration() {
    StringBuilder sb = new StringBuilder();
    int count = 0;
    for (NeuralNetConfiguration conf :
         getLayerWiseConfigurations().getConfs()) {
      sb.append(" Layer " + count++ + " conf " + conf);
    }
    log.info(sb.toString());
  }
  public void update(MultiLayerNetwork network) {
    this.defaultConfiguration = network.defaultConfiguration;
    this.input = network.input;
    this.labels = network.labels;
    this.layers = ArrayUtils.clone(network.layers);
  }
  @Override
  public double score(INDArray input, INDArray labels) {
    feedForward(input);
    setLabels(labels);
    Evaluation eval = new Evaluation();
    eval.eval(labels, labelProbabilities(input));
    return eval.f1();
  }
  @Override
  public int numLabels() {
    return labels.columns();
  }
  public double score(DataSet data) {
    feedForward(data.getFeatureMatrix());
    setLabels(data.getLabels());
    return score();
  }
  @Override
  public void fit() {
    fit(input, labels);
  }
  @Override
  public void update(INDArray gradient, String paramType) {}
  @Override
  public double score() {
    if (getOutputLayer().input() == null)
      feedForward();
    return getOutputLayer().score();
  }
  @Override
  public void computeGradientAndScore() {}
  @Override
  public void accumulateScore(double accum) {}
  public void clear() {
    for (Layer layer : layers)
      layer.clear();
    input = null;
  }
  public double score(INDArray param) {
    INDArray params = params();
    setParameters(param);
    double ret = score();
    double regCost = 0.5f * defaultConfiguration.getL2() *
                     (double)Transforms.pow(mask.mul(param), 2)
                         .sum(Integer.MAX_VALUE)
                         .element();
    setParameters(params);
    return ret + regCost;
  }
  public void merge(MultiLayerNetwork network, int batchSize) {
    if (network.layers.length != layers.length)
      throw new IllegalArgumentException(
          "Unable to merge networks that are not of equal length");
    for (int i = 0; i < getnLayers(); i++) {
      Layer n = layers[i];
      Layer otherNetwork = network.layers[i];
      n.merge(otherNetwork, batchSize);
    }
    getOutputLayer().merge(network.getOutputLayer(), batchSize);
  }
  public void setInput(INDArray input) {
    if (getLayerWiseConfigurations().getInputPreProcess(0) != null)
      this.input =
          this.layerWiseConfigurations.getInputPreProcess(0).preProcess(input);
    else
      this.input = input;
    if (this.layers == null)
      this.initializeLayers(getInput());
    else if (this.input == null)
      this.input = input;
  }
  private void initMask() { setMask(Nd4j.ones(1, pack().length())); }
  public Layer getOutputLayer() { return getLayers()[getLayers().length - 1]; }
  public void setParameters(INDArray params) {
    int idx = 0;
    for (int i = 0; i < getLayers().length; i++) {
      Layer layer = getLayers()[i];
      int range = layer.numParams();
      INDArray get = params.get(NDArrayIndex.interval(idx, range + idx));
      if (get.length() < 1)
        throw new IllegalStateException(
            "Unable to retrieve layer. No params found (length was 0");
      layer.setParams(get);
      layer.computeGradientAndScore();
      idx += range - 1;
    }
  }
  public List<INDArray> feedForwardR(List<INDArray> acts, INDArray v) {
    List<INDArray> R = new ArrayList<>();
    R.add(Nd4j.zeros(input.slices(), input.columns()));
    List<Pair<INDArray, INDArray>> vWvB = unPack(v);
    List<INDArray> W = MultiLayerUtil.weightMatrices(this);
    for (int i = 0; i < layers.length; i++) {
      String derivative = getLayers()[i].conf().getActivationFunction();
      R.add(R.get(i)
                .mmul(W.get(i))
                .addi(acts.get(i).mmul(vWvB.get(i).getFirst().addiRowVector(
                    vWvB.get(i).getSecond())))
                .muli((Nd4j.getExecutioner().execAndReturn(
                    Nd4j.getOpFactory()
                        .createTransform(derivative, acts.get(i + 1))
                        .derivative()))));
    }
    return R;
  }
  public List<INDArray> feedForwardR(INDArray v) {
    return feedForwardR(feedForward(), v);
  }
  protected List<Pair<INDArray, INDArray>> backPropGradientR(INDArray v) {
    if (mask == null)
      initMask();
    List<INDArray> deltas = computeDeltasR(v);
    List<Pair<INDArray, INDArray>> list = new ArrayList<>();
    for (int l = 0; l < getnLayers(); l++) {
      INDArray gradientChange = deltas.get(l);
      if (gradientChange.length() !=
          getLayers()[l].getParam(DefaultParamInitializer.WEIGHT_KEY).length())
        throw new IllegalStateException(
            "Gradient change not equal to weight change");
      INDArray deltaColumnSums = deltas.get(l).mean(0);
      if (deltaColumnSums.length() !=
          layers[l].getParam(DefaultParamInitializer.BIAS_KEY).length())
        throw new IllegalStateException(
            "Bias change not equal to weight change");
      list.add(new Pair<>(gradientChange, deltaColumnSums));
    }
    INDArray pack =
        pack(list)
            .addi(mask.mul(defaultConfiguration.getL2()).muli(v))
            .addi(v.mul(layerWiseConfigurations.getDampingFactor()));
    return unPack(pack);
  }
  public INDArray getLabels() { return labels; }
  public INDArray getInput() { return input; }
  public void setLabels(INDArray labels) { this.labels = labels; }
  public int getnLayers() { return layerWiseConfigurations.getConfs().size(); }
  public Layer[] getLayers() { return layers; }
  public Layer getLayer(int i) { return layers[i]; }
  public void setLayers(Layer[] layers) { this.layers = layers; }
  public INDArray getMask() { return mask; }
  public void setMask(INDArray mask) { this.mask = mask; }
}